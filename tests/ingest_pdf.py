import engram
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader
import json
import os

# ×”×’×“×¨×•×ª
PDF_PATH = "my_book.pdf"       # ×©×™× ×¤×” ×©× ×©×œ ×§×•×‘×¥ PDF ×××™×ª×™ ×©×™×© ×œ×š
INDEX_FILE = "data/book.bin"   # ××™×¤×” × ×©××•×¨ ××ª ×”×× ×•×¢ ×”×•×•×§×˜×•×¨×™
META_FILE = "data/book_meta.json" # ××™×¤×” × ×©××•×¨ ××ª ×”×˜×§×¡×˜×™×
CHUNK_SIZE = 100               # ×›××” ××™×œ×™× ×‘×›×œ ×—×ª×™×›×” (×œ× ×’×“×•×œ ××“×™ ×•×œ× ×§×˜×Ÿ ××“×™)

def main():
    PDF_PATH = input("Enter path to PDF file (default: my_book.pdf): ") or PDF_PATH
    # 1. ×˜×¢×™× ×ª ×”××•×“×œ
    print("ğŸ§  Loading AI Model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # 2. ×§×¨×™××ª ×”-PDF
    print(f"ğŸ“– Reading PDF: {PDF_PATH}...")
    if not os.path.exists(PDF_PATH):
        print("âŒ Error: Please put a PDF file in the folder and rename it to 'my_book.pdf'")
        return

    reader = PdfReader(PDF_PATH)
    full_text = ""
    for page in reader.pages:
        full_text += page.extract_text() + "\n"
    
    print(f"   Extracted {len(full_text)} characters.")

    # 3. ×—×™×ª×•×š ×œ×—×ª×™×›×•×ª (Chunking)
    # ×× ×—× ×• ×œ× ×™×›×•×œ×™× ×œ×”×›× ×™×¡ ×¡×¤×¨ ×©×œ× ×œ×•×§×˜×•×¨ ××—×“. ×—×•×ª×›×™× ×œ×¤×¡×§××•×ª.
    words = full_text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= CHUNK_SIZE:
            chunks.append(" ".join(current_chunk))
            current_chunk = [] # ××™×¤×•×¡ (××¤×©×¨ ×œ×¢×©×•×ª ×—×¤×™×¤×” - Overlap - ×œ×©×™×¤×•×¨ ×ª×•×¦××•×ª)
    
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    print(f"   Split into {len(chunks)} chunks.")

    # 4. ×™×¦×™×¨×ª ×”××™× ×“×§×¡ ×•×”×›× ×¡×ª ×”× ×ª×•× ×™×
    print("ğŸš€ Indexing to Engram...")
    db = engram.VectorIndex(384) # ××™××“ ×”××•×“×œ
    metadata = {} # ××™×œ×•×Ÿ ×œ×©××™×¨×ª ×”×˜×§×¡×˜ ×”××§×•×¨×™: ID -> Text

    for i, chunk in enumerate(chunks):
        # ×”××¨×” ×œ×•×§×˜×•×¨
        vec = model.encode(chunk).tolist()
        # ×©××™×¨×” ×‘×× ×•×¢
        db.add_vector(vec)
        # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”××§×•×¨×™ ×‘×¦×“
        metadata[i] = chunk
        
        if i % 10 == 0:
            print(f"   Processed {i}/{len(chunks)} chunks...", end="\r")

    # 5. ×©××™×¨×” ×œ×“×™×¡×§ (Persistence)
    # ×™×•×¦×¨×™× ×ª×™×§×™×™×ª data ×× ×œ× ×§×™×™××ª
    os.makedirs("data", exist_ok=True)
    
    # ×©×•××¨×™× ××ª ×”×•×§×˜×•×¨×™× (Engram)
    db.save_index(INDEX_FILE)
    
    # ×©×•××¨×™× ××ª ×”×˜×§×¡×˜×™× (JSON)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)

    print(f"\nâœ… Done! Saved index to '{INDEX_FILE}' and metadata to '{META_FILE}'")

if __name__ == "__main__":
    main()